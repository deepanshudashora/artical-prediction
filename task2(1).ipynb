{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ad sales boost Time Warner profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quarterly profits at US media giant TimeWarner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The firm, which is now one of the biggest inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time Warner said on Friday that it now owns 8%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Time Warner's fourth quarter profits were slig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TimeWarner is to restate its accounts as part ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ad sales boost Time Warner profit\n",
       "0  Quarterly profits at US media giant TimeWarner...\n",
       "1  The firm, which is now one of the biggest inve...\n",
       "2  Time Warner said on Friday that it now owns 8%...\n",
       "3  Time Warner's fourth quarter profits were slig...\n",
       "4  TimeWarner is to restate its accounts as part ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [line.rstrip() for line in open('bbc-full-text-document-classification/bbc/business/001.txt')]\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"bbc-full-text-document-classification/bbc/business/001.txt\", delimiter = '\\t', quoting =3)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0      type\n",
      "0  us govern investig two airlin us airway delta ...  business\n",
      "1  thailand becom first southern asian nation bat...  business\n",
      "2            gener motor warn expect earn year lower  business\n",
      "3  custom tri get call centr get impati quicker h...  business\n",
      "4  almost quarter million us consum complain targ...  business\n"
     ]
    }
   ],
   "source": [
    "#there a 5 folders having text files so taking care of each folder and making data frames with their files\n",
    "l = []\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "mydir = '/home/deepanshu/Downloads/bbc-full-text-document-classification/bbc/business/'\n",
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "files = [f for f in listdir(mydir) if isfile(join(mydir,f))] #a list of all the files present in that folder\n",
    "for i in files:\n",
    "    dataset= pd.read_csv( '/home/deepanshu/Downloads/bbc-full-text-document-classification/bbc/business/' + i,delimiter = '\\t', quoting =3)\n",
    "    review = list()\n",
    "    lines = dataset[: 1].values.tolist()\n",
    "    for i in lines:\n",
    "        i = i[0].lower()\n",
    "        i = re.sub(r\"[,.\\/\"\"$%^&*]\",\"\",i)\n",
    "        token = word_tokenize(i)\n",
    "        table = str.maketrans('','',string.punctuation)\n",
    "        stripped = [w.translate(table) for w in token]\n",
    "        words =[word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        ps = PorterStemmer()\n",
    "        words = [ps.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        review.append(words)\n",
    "    l = l + review\n",
    "data= pd.DataFrame(l)\n",
    "type = []\n",
    "for i in range(0,len(data[data.columns[0]])):\n",
    "    type.append(\"business\") \n",
    "data[\"type\"] = type #adding their belonging class\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0           type\n",
      "0  organis say year berlin film festiv open thurs...  entartainment\n",
      "1  hollywood star brought touch glamour london sa...  entartainment\n",
      "2  sir joshua reynold portrait omai get public ai...  entartainment\n",
      "3  russian drama return vozvrashcheni name winner...  entartainment\n",
      "4  fewer children uk follow dainti footstep dance...  entartainment\n"
     ]
    }
   ],
   "source": [
    "#repeating the same process agin on 2nd folder\n",
    "\n",
    "a = []\n",
    "mydir = '/home/deepanshu/Downloads/bbc-full-text-document-classification/bbc/entertainment/'\n",
    "fil = [f for f in listdir(mydir) if isfile(join(mydir,f))]\n",
    "for i in fil:\n",
    "    dataset= pd.read_csv('/home/deepanshu/Downloads/bbc-full-text-document-classification/bbc/entertainment/'+i, delimiter = '\\t', quoting =3)\n",
    "    revi = list()\n",
    "    lines = dataset[: 1].values.tolist()\n",
    "    for i in lines:\n",
    "        i = i[0].lower()\n",
    "        i = re.sub(r\"[,.\\/\"\"$%^&*]\",\"\",i)\n",
    "        token = word_tokenize(i)\n",
    "        table = str.maketrans('','',string.punctuation)\n",
    "        stripped = [w.translate(table) for w in token]\n",
    "        words =[word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        ps = PorterStemmer()\n",
    "        words = [ps.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        revi.append(words)\n",
    "    a = a + revi\n",
    "dataset2 = pd.DataFrame(a)  \n",
    "ty = []\n",
    "for i in range(0,len(dataset2[dataset2.columns[0]])):\n",
    "    ty.append(\"entartainment\")\n",
    "dataset2[\"type\"] = ty\n",
    "print(dataset2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0      type\n",
      "0  broadcast fix date preelect televis debat thre...  politics\n",
      "1  chancellor gordon brown deliv budget hous comm...  politics\n",
      "2  final appeal made govern not ditch reform plan...  politics\n",
      "3  risk pet children given vote could cut chang p...  politics\n",
      "4  nation insur rais labour win next elect tori l...  politics\n"
     ]
    }
   ],
   "source": [
    "b = []\n",
    "mydir = '/home/deepanshu/Downloads/bbc-full-text-document-classification/bbc/politics/'\n",
    "file3 = [f for f in listdir(mydir) if isfile(join(mydir,f))]\n",
    "for i in file3:\n",
    "    dataset= pd.read_csv('/home/deepanshu/Downloads/bbc-full-text-document-classification/bbc/politics/'+i, delimiter = '\\t', quoting =3)\n",
    "    review3 = list()\n",
    "    lines = dataset[: 1].values.tolist()\n",
    "    for i in lines:\n",
    "        i = i[0].lower()\n",
    "        i = re.sub(r\"[,.\\/\"\"$%^&*]\",\"\",i)\n",
    "        token = word_tokenize(i)\n",
    "        table = str.maketrans('','',string.punctuation)\n",
    "        stripped = [w.translate(table) for w in token]\n",
    "        words =[word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        ps = PorterStemmer()\n",
    "        words = [ps.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        review3.append(words)\n",
    "    b = b + review3\n",
    "dataset3 = pd.DataFrame(b)  \n",
    "type3 = []\n",
    "for i in range(0,len(dataset3[dataset3.columns[0]])):\n",
    "    type3.append(\"politics\")\n",
    "dataset3[\"type\"] = type3\n",
    "print(dataset3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0  type\n",
      "0  microsoft given game maker glimps new xbox consol  tech\n",
      "1  two men part huge network internet softwar pir...  tech\n",
      "2  microsoft work new version internet explor web...  tech\n",
      "3  softwar not monitor everi keystrok action perf...  tech\n",
      "4  sun microsystem launch payasyougo servic allow...  tech\n"
     ]
    }
   ],
   "source": [
    "c = []\n",
    "mydir = '/home/deepanshu/Downloads/bbc-full-text-document-classification/bbc/tech/'\n",
    "file4 = [f for f in listdir(mydir) if isfile(join(mydir,f))]\n",
    "for i in file4:\n",
    "    dataset= pd.read_csv('/home/deepanshu/Downloads/bbc-full-text-document-classification/bbc/tech/'+i, delimiter = '\\t', quoting =3)\n",
    "    review4 = list()\n",
    "    lines = dataset[: 1].values.tolist()\n",
    "    for i in lines:\n",
    "        i = i[0].lower()\n",
    "        i = re.sub(r\"[,.\\/\"\"$%^&*]\",\"\",i)\n",
    "        token = word_tokenize(i)\n",
    "        table = str.maketrans('','',string.punctuation)\n",
    "        stripped = [w.translate(table) for w in token]\n",
    "        words =[word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        ps = PorterStemmer()\n",
    "        words = [ps.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        review4.append(words)\n",
    "    c = c + review4\n",
    "dataset4 = pd.DataFrame(c)  \n",
    "type4 = []\n",
    "for i in range(0,len(dataset4[dataset4.columns[0]])):\n",
    "    type4.append(\"tech\")\n",
    "dataset4[\"type\"] = type4\n",
    "print(dataset4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0   type\n",
      "0  wale number eight michael owen say replac gare...  sport\n",
      "1   gun glori ultim success keep gunpowd dri essenti  sport\n",
      "2  martina navratilova defend decis prolong tenni...  sport\n",
      "3  world outdoor tripl jump record holder bbc pun...  sport\n",
      "4  world antidop agenc wada appeal acquitt kosta ...  sport\n"
     ]
    }
   ],
   "source": [
    "d = []\n",
    "mydir = '/home/deepanshu/Downloads/bbc-full-text-document-classification/bbc/sport/'\n",
    "file5 = [f for f in listdir(mydir) if isfile(join(mydir,f))]\n",
    "for i in file5:\n",
    "    dataset= pd.read_csv('/home/deepanshu/Downloads/bbc-full-text-document-classification/bbc/sport/'+i, delimiter = '\\t', quoting =3,encoding = \"ISO-8859-1\")\n",
    "    review5 = list()\n",
    "    lines = dataset[: 1].values.tolist()\n",
    "    for i in lines:\n",
    "        i = i[0].lower()\n",
    "        i = re.sub(r\"[,.\\/\"\"$%^&*]\",\"\",i)\n",
    "        token = word_tokenize(i)\n",
    "        table = str.maketrans('','',string.punctuation)\n",
    "        stripped = [w.translate(table) for w in token]\n",
    "        words =[word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        ps = PorterStemmer()\n",
    "        words = [ps.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        review5.append(words)\n",
    "    d = d + review5\n",
    "dataset5 = pd.DataFrame(d)\n",
    "type5 = []\n",
    "for i in range(0,len(dataset5[dataset5.columns[0]])):\n",
    "    type5.append(\"sport\")\n",
    "dataset5[\"type\"] = type5\n",
    "print(dataset5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a final dataframe with all 5 (adding them row wise)\n",
    "df = pd.concat([data,dataset2,dataset3,dataset4,dataset5],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a list for the bag of words model of all the text outputs\n",
    "corpus = l+a+b+c+d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a beg of words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'business', 'business', ..., 'sport', 'sport', 'sport'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train,y_test  = train_test_split(X,y,test_size=0.20,random_state=0)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "cr = classification_report(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'               precision    recall  f1-score   support\\n\\n     business       0.84      0.78      0.81       111\\nentartainment       0.81      0.82      0.82        74\\n     politics       0.82      0.85      0.83        86\\n        sport       0.92      0.92      0.92        93\\n         tech       0.80      0.83      0.81        81\\n\\n     accuracy                           0.84       445\\n    macro avg       0.84      0.84      0.84       445\\n weighted avg       0.84      0.84      0.84       445\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8404494382022472\n"
     ]
    }
   ],
   "source": [
    "#accuracy of model with training \n",
    "def accuracy(confusion_matrix):\n",
    "    dsum= confusion_matrix.trace()\n",
    "    all_elements = confusion_matrix.sum()\n",
    "    return  dsum/ all_elements\n",
    "\n",
    "#Creating the confusion matrix with y_val and y_pred\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "print(\"Accuracy : \", accuracy(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['politics', 'tech', 'business', 'business', 'business', 'sport',\n",
       "       'business', 'sport', 'tech', 'politics', 'business', 'politics',\n",
       "       'business', 'sport', 'business', 'entartainment', 'tech', 'sport',\n",
       "       'politics', 'tech', 'business', 'business', 'sport', 'politics',\n",
       "       'politics', 'business', 'sport', 'politics', 'politics', 'sport',\n",
       "       'entartainment', 'politics', 'tech', 'tech', 'sport', 'tech',\n",
       "       'entartainment', 'entartainment', 'politics', 'entartainment',\n",
       "       'politics', 'entartainment', 'business', 'sport', 'business',\n",
       "       'politics', 'tech', 'tech', 'sport', 'politics', 'tech', 'tech',\n",
       "       'entartainment', 'sport', 'tech', 'tech', 'entartainment',\n",
       "       'entartainment', 'tech', 'business', 'entartainment', 'politics',\n",
       "       'politics', 'entartainment', 'sport', 'politics', 'tech', 'sport',\n",
       "       'politics', 'politics', 'business', 'sport', 'sport',\n",
       "       'entartainment', 'politics', 'entartainment', 'tech', 'sport',\n",
       "       'business', 'tech', 'business', 'tech', 'sport', 'entartainment',\n",
       "       'entartainment', 'politics', 'tech', 'sport', 'business', 'tech',\n",
       "       'sport', 'tech', 'entartainment', 'business', 'sport', 'politics',\n",
       "       'tech', 'sport', 'politics', 'politics', 'business', 'sport',\n",
       "       'entartainment', 'politics', 'business', 'tech', 'entartainment',\n",
       "       'business', 'entartainment', 'business', 'tech', 'politics',\n",
       "       'sport', 'politics', 'entartainment', 'sport', 'tech',\n",
       "       'entartainment', 'business', 'tech', 'sport', 'politics', 'sport',\n",
       "       'sport', 'business', 'sport', 'tech', 'politics', 'entartainment',\n",
       "       'sport', 'business', 'sport', 'entartainment', 'business',\n",
       "       'entartainment', 'business', 'politics', 'politics', 'business',\n",
       "       'politics', 'business', 'politics', 'sport', 'business', 'sport',\n",
       "       'tech', 'business', 'business', 'politics', 'sport', 'sport',\n",
       "       'sport', 'politics', 'sport', 'politics', 'business', 'politics',\n",
       "       'tech', 'entartainment', 'tech', 'sport', 'business', 'politics',\n",
       "       'business', 'politics', 'sport', 'politics', 'sport', 'sport',\n",
       "       'politics', 'business', 'entartainment', 'entartainment',\n",
       "       'business', 'business', 'business', 'politics', 'sport',\n",
       "       'politics', 'tech', 'sport', 'sport', 'sport', 'business',\n",
       "       'business', 'politics', 'sport', 'entartainment', 'entartainment',\n",
       "       'entartainment', 'entartainment', 'sport', 'politics',\n",
       "       'entartainment', 'sport', 'business', 'business', 'politics',\n",
       "       'tech', 'sport', 'politics', 'politics', 'business', 'tech',\n",
       "       'tech', 'business', 'sport', 'politics', 'sport', 'politics',\n",
       "       'entartainment', 'politics', 'politics', 'sport', 'politics',\n",
       "       'tech', 'tech', 'business', 'sport', 'entartainment', 'politics',\n",
       "       'entartainment', 'sport', 'business', 'tech', 'business', 'sport',\n",
       "       'sport', 'tech', 'sport', 'tech', 'politics', 'sport', 'sport',\n",
       "       'entartainment', 'business', 'tech', 'business', 'tech', 'sport',\n",
       "       'business', 'entartainment', 'tech', 'sport', 'politics',\n",
       "       'business', 'business', 'sport', 'sport', 'politics', 'tech',\n",
       "       'politics', 'entartainment', 'politics', 'politics', 'tech',\n",
       "       'business', 'tech', 'sport', 'business', 'entartainment',\n",
       "       'business', 'business', 'entartainment', 'tech', 'entartainment',\n",
       "       'politics', 'sport', 'business', 'business', 'tech', 'business',\n",
       "       'sport', 'tech', 'business', 'entartainment', 'politics',\n",
       "       'politics', 'politics', 'entartainment', 'politics', 'business',\n",
       "       'tech', 'business', 'sport', 'business', 'sport', 'business',\n",
       "       'sport', 'entartainment', 'tech', 'sport', 'tech', 'sport', 'tech',\n",
       "       'entartainment', 'sport', 'politics', 'entartainment',\n",
       "       'entartainment', 'sport', 'tech', 'tech', 'business', 'sport',\n",
       "       'politics', 'politics', 'tech', 'tech', 'business', 'politics',\n",
       "       'business', 'business', 'tech', 'business', 'sport', 'tech',\n",
       "       'politics', 'business', 'tech', 'business', 'politics', 'business',\n",
       "       'entartainment', 'tech', 'tech', 'entartainment', 'sport',\n",
       "       'business', 'business', 'entartainment', 'sport', 'entartainment',\n",
       "       'business', 'politics', 'tech', 'politics', 'tech', 'sport',\n",
       "       'business', 'sport', 'sport', 'tech', 'politics', 'politics',\n",
       "       'sport', 'business', 'sport', 'entartainment', 'business',\n",
       "       'politics', 'business', 'politics', 'business', 'business',\n",
       "       'business', 'politics', 'politics', 'business', 'sport',\n",
       "       'politics', 'sport', 'sport', 'sport', 'sport', 'tech', 'tech',\n",
       "       'business', 'business', 'tech', 'sport', 'business', 'politics',\n",
       "       'tech', 'tech', 'tech', 'tech', 'entartainment', 'entartainment',\n",
       "       'business', 'politics', 'business', 'tech', 'sport',\n",
       "       'entartainment', 'politics', 'sport', 'business', 'business',\n",
       "       'sport', 'politics', 'entartainment', 'politics', 'tech', 'tech',\n",
       "       'entartainment', 'sport', 'business', 'sport', 'sport', 'tech',\n",
       "       'tech', 'sport', 'business', 'sport', 'politics', 'sport', 'tech',\n",
       "       'business', 'business', 'sport', 'entartainment', 'sport', 'tech',\n",
       "       'sport', 'business', 'sport', 'entartainment'], dtype='<U13')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the test data frame here for cleaning\n",
    "t = []\n",
    "dataset= pd.read_csv(\"classify_text.txt\", delimiter = '\\t', quoting =3)\n",
    "review35 = list()\n",
    "lines = dataset[: 1].values.tolist()\n",
    "for i in lines:\n",
    "    i = i[0].lower()\n",
    "    i = re.sub(r\"[,.\\/\"\"$%^&*]\",\"\",i)\n",
    "    token = word_tokenize(i)\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    stripped = [w.translate(table) for w in token]\n",
    "    words =[word for word in stripped if word.isalpha()]\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.discard(\"not\")\n",
    "    ps = PorterStemmer()\n",
    "    words = [ps.stem(w) for w in words if not w in stop_words]\n",
    "    words = ' '.join(words)\n",
    "    review35.append(words)\n",
    "b = b + review35\n",
    "#applying bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "io = cv.fit_transform(b).toarray()\n",
    "#prediction\n",
    "classifier.predict(io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thanks to: towards data science, reddit, youtube, documentation of nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
